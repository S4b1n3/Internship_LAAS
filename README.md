# Modelling binary neural networks with constraint programming

The objective of this project is to build deep neural networks using constraint programming and SAT. Given an input (MNIST datasets) and an architecture, the goal is to find a weight assignment that fits an objective function. The following files contain different possible modelizations where the differenciation can be made on the objective function or on the constraints. The details of each file is given as follow :

bnn-main.cc : is the file that runs the cp model.

cp_model.h : is the base class for all the models

cp_minweight_model.h : is the CP model for the full classification with min weight

cp_maxclassification_model.h : is the CP model for max classification with constraints only on the last layer

cp_maxclassification_model2.h : is the CP model for max classification with constraint on the whole network

cp_maxsum_weights_example_model.h : is the CP model for max classification with soft version of robustness constraints

cp_robust_model.h : is the CP model for the full classification with weight and robustness constraints

data.h : is the data instance class

solution.h : is the solution class

evaluation.h : is the class that tests the solution on the whole testing and training sets

## Run intructions

USAGE:

   ./bin/bnn-main  [-O <string>] [-D <string>] [-F] [-V] [-C] [-A <int>]
                   ...  [-T <double>] [-E <int>] [-K <int>] [-X <int>] [-S
                   <int>] -M <char> [--] [--version] [-h]


Where:

   -O <string>,  --output_file <string>
     Path of the output file

   -D <string>,  --strategy <string>
     The search strategy

   -F,  --evaluation
     indicates if the evaluation on the testing and training sets has to be
     done

   -V,  --check
     indicates if the solution returned has to be tested

   -C,  --product_constraints
     indicates the use of product constraints

   -A <int>,  --archi <int>  (accepted multiple times)
     Architecture of the model

   -T <double>,  --time <double>
     Time limit for the solver

   -E <int>,  --nb_examples_per_label <int>
     Number of examples per label

   -K <int>,  --k <int>
     Robustness parameter

   -X <int>,  --nb_examples <int>
     Number of examples

   -S <int>,  --seed <int>
     Seed

   -M <char>,  --index_model <char>
     (required)  Index of the model to run

   --,  --ignore_rest
     Ignores the rest of the labeled arguments following this flag.

   --version
     Displays version information and exits.

   -h,  --help
     Displays usage information and exits.


If the flag -C is written, the product constraint will be used to compute the preactivation values.

The flag -E builds a dataset composed of the user choice number of random examples from each class. The flag -X builds a dataset composed of the user choice number of random examples. If both (or no one) are used to run the model, a defaultmode is run with one example (equivalent to -X 1).

The order of the flags is not important, just be careful to write the value corresponding to a flag just after the right one.

To complete the architecture of the network, use the flag -A (or --archi) for each hidden layer. The order that the arguments are added to the command line is the order that they will be parsed and added in the architecture.

Some examples of execution :
  ./bin/bnn-main  --index_model 1 --nb_examples 1 --seed 323 --archi 1 --archi 1 --archi 1
  ./bin/bnn-main -M 2 -C -X 3 -A 16 -A 16 -V -F
  ./bin/bnn-main -M 1 -C -E 10 -D antilex_max_0


## Sampling possibilities
All the models can be run with four differents inputs :
 - Verification : No example is given in input. A random weight assignment is generated and the solutions returned by the solver for classification and activation and preactivation values are checked.
 - Random sampling : The flag -X (--nb_examples) is used. X random examples are taken from the training set and are used as inputs for the model.
 - Per label sampling : The flag -E (--nb_examples_per_label) is used. E random examples from each class are taken from the training set and are used as inputs for the model.
 - File : Some files are generated by the program get_dataset.cpp (see details below). One of these files can be used as inputs instead of reading the training set each time. The path of the input file has to be the full path or the relative path from the repository that contains the makefile.

## Additional programs

parser.cpp : This program is used after a series of experiments. His role is to read the output file of each experiment and generate a latex table with the results and statistics. There are two input parameters :
 - path of the result files
 - index of the experiment to parse

get_dataset.cpp : This program is used before calling the solver. His role is to copy some examples inside a .txt file. This file is used as an input of the model. Two types of sampling can bu used so two types of files can be generated. The first one is "pure random" sampling : a given number of example is picked from the training set. The second one is "per label" sampling : the dataset is built with a given number of randomized examples from each class. There are also two input parameters :
  - type of sampling
  - number of examples (or number of examples per label) to write is the output file

## Result files tree management

All the result files are stored in a folder "results". In this folder, there subfolders named "resultsMX" (or "resultsMX-C") where X is the index of the model. The flag -C is add if it had been used while the execution of the program.

The third level of the tree is the architecture tested by the model : each subsubfolder is named "results\_X1\_..\_Xn" where Xi is the number of neuron on the hidden layer i.

The last level of the tree is the strategy tested : the subsubsubfolders are intitled "resultslex" where lex is the name of the strategy.

The results files are contained by theses subsubsubfolders. The names of the files are defined : "resultsK.stat" where K is the number of input examples. The files can contain more than one series of experiment. They are delimited by "----------".

It is not mandatory to create the file tree before each run, the main will create it if it does not exist and won't raise an error if it exist. The parameter 4 allows the user to choose where to put this file tree. The parser will then run through the tree.
